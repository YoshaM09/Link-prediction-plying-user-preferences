{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "from scipy import spatial\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(edgelist_df):\n",
    "    G = nx.Graph()\n",
    "    for edge in list(zip(edgelist_df.user1_id, edgelist_df.user2_id)):\n",
    "        G.add_edge(edge[0], edge[1])\n",
    "    return G\n",
    "\n",
    "def create_random_edges_dict(G, n):\n",
    "    random_edges_dict = {}\n",
    "    nodes_list = list(G.nodes)\n",
    "    edges_list = list(G.edges)\n",
    "    negative_samples_percentage = 0.001\n",
    "    num_negative_samples = int(negative_samples_percentage*(len(nodes_list)*len(nodes_list)/2 - len(edges_list)))\n",
    "    print(\"Number of negative samples in the random graph:\" + str(num_negative_samples))\n",
    "    for i in range(n):\n",
    "        random_edges = []    \n",
    "        while len(random_edges) < num_negative_samples:\n",
    "            edge = random.sample(nodes_list, 2)\n",
    "            if edge[0]==edge[1] or (edge[0], edge[1]) in random_edges or (edge[1], edge[0]) in random_edges or (edge[0], edge[1]) in edges_list or (edge[1], edge[0]) in edges_list:\n",
    "                continue\n",
    "            random_edges.append((edge[0], edge[1]))\n",
    "        random_edges_dict[i] = random_edges\n",
    "    return random_edges_dict\n",
    "\n",
    "def calculate_relative_change_for_numeric_features(df, columns):\n",
    "    for column in columns:\n",
    "        df['relative_change_'+column] = df.apply(lambda x: relative_diff(x['user1_'+column], x['user2_'+column]), axis=1)\n",
    "    return df\n",
    "\n",
    "def calculate_cosine_similarity_for_vector_features(df, columns):\n",
    "    for column in columns:\n",
    "        df['cosine_similarity_'+column] = df.apply(lambda x: cosine_similarity(x['user1_'+column], x['user2_'+column]), axis=1)\n",
    "    return df\n",
    "\n",
    "def create_mask_for_categoric_features(df, columns):\n",
    "    for column in columns:\n",
    "        df['same_'+column] = (df['user1_'+column]==df['user2_'+column]).astype(int)\n",
    "    return df\n",
    "\n",
    "def calculate_jaccard_coefficient(df, G):\n",
    "    df['jaccard_coefficient'] = df.apply(lambda x: get_jaccard_coefficient(G, x['user1_user_id'], x['user2_user_id']), axis=1)\n",
    "    return df\n",
    "\n",
    "def calculate_adamic_adar_coefficient(df, G):\n",
    "    df['adamic_adar_coefficient'] = df.apply(lambda x: get_adamic_adar_index(G, x['user1_user_id'], x['user2_user_id']), axis=1)\n",
    "    return df\n",
    "\n",
    "def calculate_common_neighbors(df, G):\n",
    "    df['common_neighbors'] = df.apply(lambda x: get_common_neighbors(G, x['user1_user_id'], x['user2_user_id']), axis=1)\n",
    "    return df\n",
    "\n",
    "def relative_diff(x, y):\n",
    "    if x is None or y is None or x==-1 or y==-1:\n",
    "        return None\n",
    "    if max(x,y) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return abs((abs(x)-abs(y))/max(x,y))\n",
    "\n",
    "def cosine_similarity(list1, list2):\n",
    "    return 1 - spatial.distance.cosine(list1, list2)\n",
    "\n",
    "def get_jaccard_coefficient(G, user1, user2):\n",
    "    return list(nx.jaccard_coefficient(G, [(user1, user2)]))[0][2]\n",
    "\n",
    "def get_adamic_adar_index(G, user1, user2):\n",
    "    return list(nx.adamic_adar_index(G, [(user1, user2)]))[0][2]\n",
    "\n",
    "def get_common_neighbors(G, user1, user2):\n",
    "    return len(list(nx.common_neighbors(G, user1, user2)))\n",
    "\n",
    "def extend_users_df_with_relational_features(users_df):\n",
    "    users_df['freebase_nmf_array'] = users_df[['freebase_nmf_0', 'freebase_nmf_1', 'freebase_nmf_2', 'freebase_nmf_3', 'freebase_nmf_4', 'freebase_nmf_5', 'freebase_nmf_6', 'freebase_nmf_7', 'freebase_nmf_8', 'freebase_nmf_9', 'freebase_nmf_10', 'freebase_nmf_11', 'freebase_nmf_12', 'freebase_nmf_13', 'freebase_nmf_14', 'freebase_nmf_15', 'freebase_nmf_16', 'freebase_nmf_17', 'freebase_nmf_18', 'freebase_nmf_19']].values.tolist()\n",
    "    users_df['allmusic_genre_array'] = users_df[['allmusic_rnb', 'allmusic_rap', 'allmusic_electronic', 'allmusic_rock', 'allmusic_new age', 'allmusic_classical', 'allmusic_reggae', 'allmusic_blues', 'allmusic_country', 'allmusic_world', 'allmusic_folk', 'allmusic_easy listening', 'allmusic_jazz', 'allmusic_vocal', \"allmusic_children's\", 'allmusic_punk', 'allmusic_alternative', 'allmusic_spoken word', 'allmusic_pop', 'allmusic_heavy metal']].values.tolist()\n",
    "    users_df['UAM_nmf_array'] = users_df[['UAM_nmf_0', 'UAM_nmf_1', 'UAM_nmf_2', 'UAM_nmf_3', 'UAM_nmf_4', 'UAM_nmf_5', 'UAM_nmf_6', 'UAM_nmf_7', 'UAM_nmf_8', 'UAM_nmf_9', 'UAM_nmf_10', 'UAM_nmf_11', 'UAM_nmf_12', 'UAM_nmf_13', 'UAM_nmf_14', 'UAM_nmf_15', 'UAM_nmf_16', 'UAM_nmf_17', 'UAM_nmf_18', 'UAM_nmf_19']].values.tolist()\n",
    "    users_df['allmusic_genre_array'] = users_df['allmusic_genre_array'].apply(lambda x: np.array(x))\n",
    "    users_df['freebase_nmf_array'] = users_df['freebase_nmf_array'].apply(lambda x: np.array(x))\n",
    "    users_df['UAM_nmf_array'] = users_df['UAM_nmf_array'].apply(lambda x: np.array(x))\n",
    "    return users_df\n",
    "\n",
    "def create_links_df(G, random_edges, users_df):\n",
    "    edges_list = list(G.edges)\n",
    "    users1_df = pd.DataFrame([x[0] for x in edges_list]).rename(columns={0:'user_id'}).merge(users_df, how='left', on='user_id').add_prefix('user1_')\n",
    "    users2_df = pd.DataFrame([x[1] for x in edges_list]).rename(columns={0:'user_id'}).merge(users_df, how='left', on='user_id').add_prefix('user2_')\n",
    "    friendship_df = pd.concat([users1_df, users2_df.set_index(users1_df.index)], axis=1)\n",
    "    friendship_df['label'] = 1\n",
    "    users1_df = pd.DataFrame([x[0] for x in random_edges]).rename(columns={0:'user_id'}).merge(users_df, how='left', on='user_id').add_prefix('user1_')\n",
    "    users2_df = pd.DataFrame([x[1] for x in random_edges]).rename(columns={0:'user_id'}).merge(users_df, how='left', on='user_id').add_prefix('user2_')\n",
    "    no_friendship_df = pd.concat([users1_df, users2_df.set_index(users1_df.index)], axis=1)\n",
    "    no_friendship_df['label'] = 0\n",
    "    links_df = pd.concat([friendship_df, no_friendship_df], axis=0, ignore_index=True)\n",
    "    links_df = calculate_relative_change_for_numeric_features(links_df, ['age','playcount_lognorm','novelty_artist_avg_month','novelty_artist_avg_6months','novelty_artist_avg_year','mainstreaminess_avg_month','mainstreaminess_avg_6months','mainstreaminess_avg_year','mainstreaminess_global','relative_le_per_weekday1','relative_le_per_weekday2','relative_le_per_weekday3','relative_le_per_weekday4','relative_le_per_weekday5','relative_le_per_weekday6','relative_le_per_weekday7','relative_le_per_hour0','relative_le_per_hour1','relative_le_per_hour2','relative_le_per_hour3','relative_le_per_hour4','relative_le_per_hour5','relative_le_per_hour6','relative_le_per_hour7','relative_le_per_hour8','relative_le_per_hour9','relative_le_per_hour10','relative_le_per_hour11','relative_le_per_hour12','relative_le_per_hour13','relative_le_per_hour14','relative_le_per_hour15','relative_le_per_hour16','relative_le_per_hour17','relative_le_per_hour18','relative_le_per_hour19','relative_le_per_hour20','relative_le_per_hour21','relative_le_per_hour22','relative_le_per_hour23','cnt_listeningevents_lognorm','cnt_distinct_tracks_lognorm','cnt_distinct_artists_lognorm','cnt_listeningevents_per_week_lognorm','allmusic_weighted_average_diversity','allmusic_genre_coverage_diversity','allmusic_entropy_diversity','freebase_weighted_average_diversity','freebase_genre_coverage_diversity','freebase_entropy_diversity'])\n",
    "    links_df = create_mask_for_categoric_features(links_df, ['country','gender','age_group','user_groups_freebase_weighted_average_diversity','user_groups_freebase_genre_coverage_diversity','user_groups_freebase_entropy_diversity','user_groups_allmusic_weighted_average_diversity','user_groups_allmusic_genre_coverage_diversity','user_groups_allmusic_entropy_diversity','user_groups_cnt_listeningevents_lognorm','user_groups_cnt_distinct_tracks_lognorm','user_groups_cnt_distinct_artists_lognorm','user_groups_cnt_listeningevents_per_week_lognorm','user_groups_playcount_lognorm','user_groups_novelty_artist_avg_month','user_groups_novelty_artist_avg_6months','user_groups_novelty_artist_avg_year','user_groups_mainstreaminess_avg_month','user_groups_mainstreaminess_avg_6months','user_groups_mainstreaminess_avg_year','user_groups_mainstreaminess_global'])\n",
    "    links_df = calculate_cosine_similarity_for_vector_features(links_df, ['allmusic_genre_array', 'freebase_nmf_array', 'UAM_nmf_array'])\n",
    "    links_df = links_df.select_dtypes(exclude=['object'])\n",
    "    links_df = calculate_jaccard_coefficient(links_df, G)\n",
    "    links_df = calculate_adamic_adar_coefficient(links_df, G)\n",
    "    links_df = calculate_common_neighbors(links_df, G)\n",
    "    return links_df\n",
    "\n",
    "def create_and_store_dfs(G, random_edges_dict, users_df):\n",
    "    for i in range(len(random_edges_dict)):\n",
    "        links_df = create_links_df(G, random_edges_dict[i], users_df)\n",
    "        outdir = '../data/dataframes/links_dfs/'+ str(i)\n",
    "        if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "        links_df.to_csv(outdir + '/links_df_full.csv')\n",
    "        network_features_df = links_df[['user1_user_id', 'user2_user_id', 'jaccard_coefficient', 'adamic_adar_coefficient', 'common_neighbors', 'label']]\n",
    "        demographic_features_df = pd.concat([links_df.filter(regex=(\"user1_user_id\")), links_df.filter(regex=(\"user2_user_id\")), links_df.filter(regex=(\".*gender.*\")), links_df.filter(regex=(\".*age.*\")), links_df.filter(regex=(\".*country.*\")), links_df.label], axis=1).drop(columns=['user1_allmusic_country', 'user2_allmusic_country'])\n",
    "        demographic_features_df = demographic_features_df[demographic_features_df.columns.drop(list(demographic_features_df.filter(regex='allmusic')))]\n",
    "        demographic_features_df = demographic_features_df[demographic_features_df.columns.drop(list(demographic_features_df.filter(regex='freebase')))]\n",
    "        genre_features_df = pd.concat([links_df.filter(regex=(\"user1_user_id\")), links_df.filter(regex=(\"user2_user_id\")), links_df.filter(regex=(\".*allmusic.*\")), links_df.filter(regex=(\".*freebase.*\")), links_df.label], axis=1)\n",
    "        genre_features_df = genre_features_df[genre_features_df.columns.drop(list(genre_features_df.filter(regex='diversity')))]\n",
    "        listening_profile_features_df = pd.concat([links_df.filter(regex=(\"user1_user_id\")), links_df.filter(regex=(\"user2_user_id\")), links_df.filter(regex=(\".*UAM.*\")), links_df.label], axis=1)\n",
    "        listening_characteristics_features_df = pd.concat([links_df.filter(regex=(\"user1_user_id\")), links_df.filter(regex=(\"user2_user_id\")), links_df.filter(regex=(\".*mainstreaminess.*\")), links_df.filter(regex=(\".*novelty.*\")), links_df.filter(regex=(\".*diversity.*\")), links_df.filter(regex=(\".*cnt.*\")), links_df.filter(regex=(\".*playcount.*\")), links_df.label], axis=1)\n",
    "        network_features_df.to_csv('../data/dataframes/links_dfs/' + str(i) + '/links_df_network_features.csv')\n",
    "        demographic_features_df.to_csv('../data/dataframes/links_dfs/' + str(i) + '/links_df_demographic_features.csv')\n",
    "        genre_features_df.to_csv('../data/dataframes/links_dfs/' + str(i) + '/links_df_genre_features.csv')\n",
    "        listening_profile_features_df.to_csv('../data/dataframes/links_dfs/' + str(i) + '/links_df_listening_profile_features.csv')\n",
    "        listening_characteristics_features_df.to_csv('../data/dataframes/links_dfs/' + str(i) + '/links_df_listening_characteristics_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lfm1b_user_info_filepath = '../data/raw/LFM-1b_social/LFM-1b_users.txt'\n",
    "lfm1b_user_additional_info_filepath = '../data/raw/LFM-1b/LFM-1b_users_additional.txt'\n",
    "lfm1b_user_genres_allmusic_filepath = '../data/raw/LFM-1b_UGP/LFM-1b_UGP_weightedPC_allmusic.txt'\n",
    "lfm1b_user_genres_freebase_filepath = '../data/raw/LFM-1b_UGP/LFM-1b_UGP_weightedPC_freebase.txt'\n",
    "input_edgelist_csv_filepath = '../data/raw/LFM-1b_social/LFM-1b_social_ties.txt'\n",
    "lfm1b_user_artists_LEs_filepath = '../data/raw/LFM-1b/LFM-1b_LEs.mat'\n",
    "users_df_filepath = ('../data/dataframes/users_dfs/users_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative samples in the random graph:69446\n"
     ]
    }
   ],
   "source": [
    "lfm1b_users_df = pd.read_csv(lfm1b_user_info_filepath, sep='\\t')\n",
    "edgelist_df = pd.read_csv(input_edgelist_csv_filepath, sep='\\t')\n",
    "G = create_graph(edgelist_df)\n",
    "\n",
    "num_datasets = 1\n",
    "random_edges_dict = create_random_edges_dict(G, num_datasets)\n",
    "\n",
    "users_df = pd.read_csv(users_df_filepath)\n",
    "users_df = extend_users_df_with_relational_features(users_df)\n",
    "\n",
    "users_df_without_missing_values = pd.read_csv('../data/dataframes/users_dfs/users_df_no_missing_values.csv', index_col=0)\n",
    "users_df_without_missing_values = extend_users_df_with_relational_features(users_df_without_missing_values)\n",
    "\n",
    "create_and_store_dfs(G, random_edges_dict, users_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "links_df = pd.read_csv('../data/dataframes/links_dfs/0/links_df_full.csv', index_col=False).drop(columns=['Unnamed: 0'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
