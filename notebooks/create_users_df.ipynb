{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from nonnegfac.nmf import NMF_ANLS_BLOCKPIVOT\n",
    "from sklearn.preprocessing import normalize\n",
    "import h5py\n",
    "from scipy import sparse\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def download_and_unpack_zip_file(url, path, targetdir):\n",
    "    wget.download(url, path)\n",
    "    shutil.unpack_archive(path, targetdir)\n",
    "\n",
    "def prepare_directories():\n",
    "    directories_to_create = ['downloaded', 'raw', 'dataframes', 'tmp']\n",
    "    for directory in directories_to_create:\n",
    "        dirname = '/'.join(['../data', directory])\n",
    "        Path(dirname).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def download_datasets():\n",
    "    lfm1b_dataset_url = 'http://drive.jku.at/ssf/s/readFile/share/1056/266403063659030189/publicLink/LFM-1b.zip'\n",
    "    lfm1b_ugp_dataset_url = 'http://www.cp.jku.at/datasets/LFM-1b/LFM-1b_UGP.zip'\n",
    "    lfm1b_social_dataset_url = 'https://zenodo.org/record/5585638/files/LFM-1b_social.zip?download=1'\n",
    "\n",
    "    targetdir = '../data/raw'\n",
    "    print('Downloading and unpacking LFM-1b dataset (~8GB)...')\n",
    "    download_and_unpack_zip_file(lfm1b_dataset_url, '../data/downloaded/LFM-1b.zip', targetdir)\n",
    "    print('Finished.')\n",
    "\n",
    "    print('Downloading and unpacking LFM-1b_UGP dataset (~166MB)...')\n",
    "    download_and_unpack_zip_file(lfm1b_ugp_dataset_url, '../data/downloaded/LFM-1b_UGP.zip', targetdir)\n",
    "    print('Finished.')\n",
    "\n",
    "    print('Downloading and unpacking LFM-1b_social dataset (~2MB)...')\n",
    "    download_and_unpack_zip_file(lfm1b_social_dataset_url, '../data/downloaded/LFM-1b_social.zip', targetdir)\n",
    "    print('Finished.')\n",
    "\n",
    "\n",
    "def create_lfm1b_users_df(lfm1b_users_filepath):\n",
    "    lfm1b_users_df = pd.read_csv(lfm1b_users_filepath, sep='\\t')\n",
    "    lfm1b_users_df.to_csv('../data/raw/LFM-1b_social/LFM-1b_users.txt', sep = '\\t', index=False)\n",
    "    return lfm1b_users_df\n",
    "\n",
    "\n",
    "def create_users_df(edgelist_df,\n",
    "                    lfm1b_user_info_filepath,\n",
    "                    lfm1b_user_additional_info_filepath,\n",
    "                    lfm1b_user_genres_allmusic_filepath,\n",
    "                    replace_missing_values_with_mean=False):\n",
    "    users_list = list(edgelist_df.user1_id.unique())\n",
    "    users_list.extend(list(edgelist_df.user2_id.unique()))\n",
    "    users_list = list(set(users_list))\n",
    "    users_df = pd.DataFrame(users_list).rename(columns={0:'user_id'})\n",
    "    lfm1b_user_info_df = pd.read_csv(lfm1b_user_info_filepath, sep='\\t').drop(columns=['registered_unixtime'])\n",
    "    users_df = users_df.merge(lfm1b_user_info_df, how='left', on='user_id')\n",
    "    users_df['age_group'] = users_df['age'].apply(get_age_group)\n",
    "    users_df.loc[users_df['gender'].isnull(), 'gender'] = 'n'\n",
    "    users_df.loc[users_df['country'].isnull(), 'country'] = 'N/A'\n",
    "    users_df.loc[users_df['playcount'] == 0, 'playcount'] = 1\n",
    "    users_df['playcount_lognorm'] = np.log(users_df['playcount'])\n",
    "    users_df = users_df.drop(columns=['playcount'])\n",
    "    lfm1b_user_additional_info_df = pd.read_csv(lfm1b_user_additional_info_filepath, sep='\\t').rename(columns={'user-id':'user_id'}).replace('?', None).astype({'novelty_artist_avg_month': 'float64',\n",
    "                            'novelty_artist_avg_year': 'float64',\n",
    "                            'mainstreaminess_avg_6months':'float64',\n",
    "                            'relative_le_per_weekday1':'float64',\n",
    "                            'relative_le_per_weekday2':'float64',\n",
    "                            'relative_le_per_weekday3':'float64',\n",
    "                            'relative_le_per_weekday4':'float64',\n",
    "                            'relative_le_per_weekday5':'float64',\n",
    "                            'relative_le_per_weekday6':'float64',\n",
    "                            'relative_le_per_weekday7':'float64',\n",
    "                            'relative_le_per_hour0':'float64',\n",
    "                            'relative_le_per_hour1':'float64',\n",
    "                            'relative_le_per_hour2':'float64',\n",
    "                            'relative_le_per_hour3':'float64',\n",
    "                            'relative_le_per_hour4':'float64',\n",
    "                            'relative_le_per_hour5':'float64',\n",
    "                            'relative_le_per_hour6':'float64',\n",
    "                            'relative_le_per_hour7':'float64',\n",
    "                            'relative_le_per_hour8':'float64',\n",
    "                            'relative_le_per_hour9':'float64',\n",
    "                            'relative_le_per_hour10':'float64',\n",
    "                            'relative_le_per_hour11':'float64',\n",
    "                            'relative_le_per_hour12':'float64',\n",
    "                            'relative_le_per_hour13':'float64',\n",
    "                            'relative_le_per_hour14':'float64',\n",
    "                            'relative_le_per_hour15':'float64',\n",
    "                            'relative_le_per_hour16':'float64',\n",
    "                            'relative_le_per_hour17':'float64',\n",
    "                            'relative_le_per_hour18':'float64',\n",
    "                            'relative_le_per_hour19':'float64',\n",
    "                            'relative_le_per_hour20':'float64',\n",
    "                            'relative_le_per_hour21':'float64',\n",
    "                            'relative_le_per_hour22':'float64',\n",
    "                            'relative_le_per_hour23':'float64'})\n",
    "    users_df = users_df.merge(lfm1b_user_additional_info_df, how='left', on='user_id')\n",
    "    users_df.loc[users_df['cnt_listeningevents'] == 0, 'cnt_listeningevents'] = 1\n",
    "    users_df['cnt_listeningevents_lognorm'] = np.log(users_df['cnt_listeningevents'])\n",
    "    users_df = users_df.drop(columns=['cnt_listeningevents'])\n",
    "    users_df.loc[users_df['cnt_distinct_tracks'] == 0, 'cnt_distinct_tracks'] = 1\n",
    "    users_df['cnt_distinct_tracks_lognorm'] = np.log(users_df['cnt_distinct_tracks'])\n",
    "    users_df = users_df.drop(columns=['cnt_distinct_tracks'])\n",
    "    users_df.loc[users_df['cnt_distinct_artists'] == 0, 'cnt_distinct_artists'] = 1\n",
    "    users_df['cnt_distinct_artists_lognorm'] = np.log(users_df['cnt_distinct_artists'])\n",
    "    users_df = users_df.drop(columns=['cnt_distinct_artists'])\n",
    "    users_df.loc[users_df['cnt_listeningevents_per_week'] == 0, 'cnt_listeningevents_per_week'] = 1\n",
    "    users_df['cnt_listeningevents_per_week_lognorm'] = np.log(users_df['cnt_listeningevents_per_week'])\n",
    "    users_df = users_df.drop(columns=['cnt_listeningevents_per_week'])\n",
    "    dummies = pd.get_dummies(users_df.country, prefix='country')\n",
    "    users_df =  pd.concat([users_df, dummies.set_index(users_df.index)], axis=1)\n",
    "    dummies = pd.get_dummies(users_df.gender, prefix='gender')\n",
    "    users_df = pd.concat([users_df, dummies.set_index(users_df.index)], axis=1)\n",
    "    dummies = pd.get_dummies(users_df.age_group, prefix='age_group')\n",
    "    users_df = pd.concat([users_df, dummies.set_index(users_df.index)], axis=1)\n",
    "\n",
    "    lfm1b_user_genres_allmusic_df = pd.read_csv(lfm1b_user_genres_allmusic_filepath, sep='\\t')\n",
    "    if os.path.isfile('../data/tmp/allmusic_diversity_df.csv'):\n",
    "        allmusic_diversity_df = pd.read_csv('../data/tmp/allmusic_diversity_df.csv', index_col=0)\n",
    "    else:\n",
    "        allmusic_diversity_df = create_allmusic_diversity_df(lfm1b_user_genres_allmusic_df, users_df)\n",
    "    users_df = users_df.merge(allmusic_diversity_df, how='left', on='user_id')\n",
    "\n",
    "    lfm1b_user_genres_allmusic_df_user_ids = pd.DataFrame(lfm1b_user_genres_allmusic_df.user_id)\n",
    "    lfm1b_user_genres_allmusic_df_genres = pd.DataFrame(lfm1b_user_genres_allmusic_df.drop(['user_id'], axis=1))\n",
    "    lfm1b_user_genres_allmusic_df_genres = lfm1b_user_genres_allmusic_df_genres.div(lfm1b_user_genres_allmusic_df_genres.max(axis=1), axis=0).add_prefix('allmusic_')\n",
    "    lfm1b_user_genres_allmusic_df = pd.concat([lfm1b_user_genres_allmusic_df_user_ids, lfm1b_user_genres_allmusic_df_genres.set_index(lfm1b_user_genres_allmusic_df_user_ids.index)], axis=1)\n",
    "    users_df = users_df.merge(lfm1b_user_genres_allmusic_df, how='left', on='user_id')\n",
    "\n",
    "    lfm1b_user_genres_freebase_df = pd.read_csv(lfm1b_user_genres_freebase_filepath, sep='\\t')\n",
    "    if os.path.isfile('../data/tmp/freebase_diversity_df.csv'):\n",
    "        freebase_diversity_df = pd.read_csv('../data/tmp/freebase_diversity_df.csv', index_col=0)\n",
    "    else:\n",
    "        freebase_diversity_df = create_freebase_diversity_df(lfm1b_user_genres_freebase_df, users_df)\n",
    "    users_df = users_df.merge(freebase_diversity_df, how='left', on='user_id')\n",
    "\n",
    "    if os.path.isfile('../data/tmp/freebase_genres_matrix_nmf_df.csv'):\n",
    "        freebase_genres_matrix_nmf_df = pd.read_csv('../data/tmp/freebase_genres_matrix_nmf_df.csv', index_col=0)\n",
    "    else:\n",
    "        freebase_genres_matrix_nmf_df = create_freebase_genres_matrix_nmf_df(lfm1b_user_genres_freebase_df)\n",
    "    users_df = users_df.merge(freebase_genres_matrix_nmf_df, how='left', on='user_id')\n",
    "\n",
    "    if os.path.isfile('../data/tmp/UAM_normalized_nmf_df.csv'):\n",
    "        UAM_normalized_nmf_df = pd.read_csv('../data/tmp/UAM_normalized_nmf_df.csv', index_col=0)\n",
    "    else:\n",
    "        UAM_normalized_nmf_df = create_UAM_normalized_nmf_df(lfm1b_user_artists_LEs_filepath)\n",
    "    users_df = users_df.merge(UAM_normalized_nmf_df, how='left', on='user_id')\n",
    "\n",
    "    if replace_missing_values_with_mean == True:\n",
    "        users_df = fill_missing_values(users_df)\n",
    "\n",
    "    column_to_convert_to_user_groups = [\n",
    "        'freebase_weighted_average_diversity',\n",
    "        'freebase_genre_coverage_diversity',\n",
    "        'freebase_entropy_diversity',\n",
    "        'allmusic_weighted_average_diversity',\n",
    "        'allmusic_genre_coverage_diversity',\n",
    "        'allmusic_entropy_diversity',\n",
    "        'cnt_listeningevents_lognorm',\n",
    "        'cnt_distinct_tracks_lognorm',\n",
    "        'cnt_distinct_artists_lognorm',\n",
    "        'cnt_listeningevents_per_week_lognorm',\n",
    "        'playcount_lognorm',\n",
    "        'novelty_artist_avg_month',\n",
    "        'novelty_artist_avg_6months',\n",
    "        'novelty_artist_avg_year',\n",
    "        'mainstreaminess_avg_month',\n",
    "        'mainstreaminess_avg_6months',\n",
    "        'mainstreaminess_avg_year',\n",
    "        'mainstreaminess_global']\n",
    "    for column in column_to_convert_to_user_groups:\n",
    "        users_df = divide_column_into_user_groups_and_merge(users_df, column)\n",
    "        dummies = pd.get_dummies(users_df['user_groups_'+column], prefix='user_groups_'+column)\n",
    "        users_df = pd.concat([users_df, dummies.set_index(users_df.index)], axis=1)\n",
    "\n",
    "    outdir = '../data/dataframes/users_dfs'\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "    if replace_missing_values_with_mean == True:\n",
    "        users_df.to_csv('../data/dataframes/users_dfs/users_df_no_missing_values.csv', index=False)\n",
    "    else:\n",
    "        users_df.to_csv('../data/dataframes/users_dfs/users_df.csv', index=False)\n",
    "\n",
    "def create_UAM_normalized_nmf_df(lfm1b_user_artists_LEs_filepath):\n",
    "    UAM, UAM_user_idx, UAM_artist_idx, user_ids, artist_ids = read_UAM(lfm1b_user_artists_LEs_filepath)\n",
    "    user_ids_df = pd.DataFrame(user_ids).rename(columns={0:'user_id'})\n",
    "    UAM = sparse.csr_matrix(UAM)\n",
    "    UAM_normalized = normalize(UAM, norm='max', axis=1)\n",
    "    UAM_normalized_nmf = create_NMF_embeddings(UAM_normalized, 20)\n",
    "    UAM_normalized_nmf_df = pd.DataFrame(UAM_normalized_nmf).add_prefix('UAM_nmf_')\n",
    "    UAM_normalized_nmf_df = pd.concat([user_ids_df, UAM_normalized_nmf_df], axis=1)\n",
    "    UAM_normalized_nmf_df.to_csv('../data/tmp/UAM_normalized_nmf_df.csv', index=False)\n",
    "    return UAM_normalized_nmf_df\n",
    "\n",
    "def create_freebase_genres_matrix_nmf_df(lfm1b_user_genres_freebase_df):\n",
    "    lfm1b_user_genres_freebase_df_user_ids = pd.DataFrame(lfm1b_user_genres_freebase_df.user_id)\n",
    "    lfm1b_user_genres_freebase_df_genres = pd.DataFrame(lfm1b_user_genres_freebase_df.drop(['user_id'], axis=1))\n",
    "    lfm1b_user_genres_freebase_df_genres = lfm1b_user_genres_freebase_df_genres.add_prefix('freebase_')\n",
    "    lfm1b_user_genres_freebase_df_genres = pd.concat([lfm1b_user_genres_freebase_df_user_ids, lfm1b_user_genres_freebase_df_genres.set_index(lfm1b_user_genres_freebase_df_user_ids.index)], axis=1)\n",
    "    freebase_genre_matrix = scipy.sparse.csr_matrix(lfm1b_user_genres_freebase_df_genres.fillna(0).values)\n",
    "    freebase_genre_matrix_normalized = normalize(freebase_genre_matrix, norm='max', axis=1)\n",
    "    freebase_genres_matrix_nmf = create_NMF_embeddings(freebase_genre_matrix_normalized, 20)\n",
    "    freebase_genres_matrix_nmf_df = pd.DataFrame(freebase_genres_matrix_nmf).add_prefix('freebase_nmf_')\n",
    "    freebase_genres_matrix_nmf_df = pd.concat([freebase_genres_matrix_nmf_df, lfm1b_user_genres_freebase_df_user_ids], axis=1)\n",
    "    freebase_genres_matrix_nmf_df.to_csv('../data/tmp/freebase_genres_matrix_nmf_df.csv', index=False)\n",
    "    return freebase_genres_matrix_nmf_df\n",
    "\n",
    "def create_freebase_diversity_df(lfm1b_user_genres_freebase_df, users_df):\n",
    "    user_ids = list(users_df.user_id.values)\n",
    "    diversity_weighted_average_list = []\n",
    "    diversity_genre_coverage_list = []\n",
    "    entropy_diversity_list = []\n",
    "    for user_id in user_ids:\n",
    "        try:\n",
    "            user_genres = lfm1b_user_genres_freebase_df[lfm1b_user_genres_freebase_df.user_id==user_id].drop(['user_id'], axis=1).values[0]\n",
    "            user_genres_normalized = user_genres/np.max(user_genres)\n",
    "            diversity_weighted_average = np.sum(user_genres_normalized)/len(user_genres_normalized)\n",
    "\n",
    "            diversity_genre_coverage = len(user_genres[user_genres > 0])/len(user_genres)\n",
    "            user_genre_counts = dict(zip(list(lfm1b_user_genres_freebase_df[lfm1b_user_genres_freebase_df.user_id==user_id].drop(['user_id'], axis=1).columns),\n",
    "                                     list(lfm1b_user_genres_freebase_df[lfm1b_user_genres_freebase_df.user_id==user_id].drop(['user_id'], axis=1).values[0])))\n",
    "            labels = []\n",
    "            for key in user_genre_counts:\n",
    "                labels.extend([key] * user_genre_counts[key])\n",
    "            entropy_diversity = entropy_label_distribution(labels)\n",
    "        except:\n",
    "            diversity_weighted_average = None\n",
    "            diversity_genre_coverage = None\n",
    "            entropy_diversity = None\n",
    "        diversity_weighted_average_list.append(diversity_weighted_average)\n",
    "        diversity_genre_coverage_list.append(diversity_genre_coverage)\n",
    "        entropy_diversity_list.append(entropy_diversity)\n",
    "    freebase_diversity_df = pd.DataFrame(\n",
    "        {'user_id': user_ids,\n",
    "         'freebase_weighted_average_diversity': diversity_weighted_average_list,\n",
    "         'freebase_genre_coverage_diversity': diversity_genre_coverage_list,\n",
    "         'freebase_entropy_diversity': entropy_diversity_list\n",
    "        })\n",
    "    freebase_diversity_df.to_csv('../data/tmp/freebase_diversity_df.csv', index=False)\n",
    "    return freebase_diversity_df\n",
    "\n",
    "def create_allmusic_diversity_df(lfm1b_user_genres_allmusic_df, users_df):\n",
    "    user_ids = list(users_df.user_id.values)\n",
    "    diversity_weighted_average_list = []\n",
    "    diversity_genre_coverage_list = []\n",
    "    entropy_diversity_list = []\n",
    "    for user_id in user_ids:\n",
    "        try:\n",
    "            user_genres = lfm1b_user_genres_allmusic_df[lfm1b_user_genres_allmusic_df.user_id==user_id].drop(['user_id'], axis=1).values[0]\n",
    "            user_genres_normalized = user_genres/np.max(user_genres)\n",
    "            diversity_weighted_average = np.sum(user_genres_normalized)/len(user_genres_normalized)\n",
    "\n",
    "            diversity_genre_coverage = len(user_genres[user_genres > 0])/len(user_genres)\n",
    "            user_genre_counts = dict(zip(list(lfm1b_user_genres_allmusic_df[lfm1b_user_genres_allmusic_df.user_id==user_id].drop(['user_id'], axis=1).columns),\n",
    "                                     list(lfm1b_user_genres_allmusic_df[lfm1b_user_genres_allmusic_df.user_id==user_id].drop(['user_id'], axis=1).values[0])))\n",
    "            labels = []\n",
    "            for key in user_genre_counts:\n",
    "                labels.extend([key] * user_genre_counts[key])\n",
    "            entropy_diversity = entropy_label_distribution(labels)\n",
    "        except:\n",
    "            diversity_weighted_average = None\n",
    "            diversity_genre_coverage = None\n",
    "            entropy_diversity = None\n",
    "        diversity_weighted_average_list.append(diversity_weighted_average)\n",
    "        diversity_genre_coverage_list.append(diversity_genre_coverage)\n",
    "        entropy_diversity_list.append(entropy_diversity)\n",
    "    allmusic_diversity_df = pd.DataFrame(\n",
    "        {'user_id': user_ids,\n",
    "         'allmusic_weighted_average_diversity': diversity_weighted_average_list,\n",
    "         'allmusic_genre_coverage_diversity': diversity_genre_coverage_list,\n",
    "         'allmusic_entropy_diversity': entropy_diversity_list\n",
    "        })\n",
    "    allmusic_diversity_df.to_csv('../data/tmp/allmusic_diversity_df.csv', index=False)\n",
    "    return allmusic_diversity_df\n",
    "\n",
    "def get_age_group(age):\n",
    "    if age == -1:\n",
    "        return '-1'\n",
    "    elif age < 5:\n",
    "        return '0-4'\n",
    "    elif age < 10:\n",
    "        return '5-9'\n",
    "    elif age < 15:\n",
    "        return '10-14'\n",
    "    elif age < 20:\n",
    "        return '15-19'\n",
    "    elif age < 25:\n",
    "        return '20-24'\n",
    "    elif age < 30:\n",
    "        return '25-29'\n",
    "    elif age < 35:\n",
    "        return '30-34'\n",
    "    elif age < 40:\n",
    "        return '35-39'\n",
    "    elif age < 45:\n",
    "        return '40-44'\n",
    "    elif age < 50:\n",
    "        return '45-49'\n",
    "    elif age < 55:\n",
    "        return '50-54'\n",
    "    elif age < 60:\n",
    "        return '55-59'\n",
    "    elif age < 65:\n",
    "        return '60-64'\n",
    "    elif age < 70:\n",
    "        return '65-69'\n",
    "    elif age < 75:\n",
    "        return '70-74'\n",
    "    elif age < 80:\n",
    "        return '75-79'\n",
    "    elif age >=80:\n",
    "        return '80+'\n",
    "\n",
    "def create_NMF_embeddings(input_matrix, dim):\n",
    "    W, _, info = NMF_ANLS_BLOCKPIVOT().run(input_matrix, dim, max_iter=100)\n",
    "    return W\n",
    "\n",
    "def read_UAM(m_file):\n",
    "    mf = h5py.File(m_file, 'r')\n",
    "    user_ids = np.array(mf.get('idx_users')).astype(np.int64)\n",
    "    artist_ids = np.array(mf.get('idx_artists')).astype(np.int64)\n",
    "    UAM = sparse.csr_matrix((mf['/LEs/'][\"data\"],\n",
    "                             mf['/LEs/'][\"ir\"],\n",
    "                             mf['/LEs/'][\"jc\"])).transpose() \n",
    "    UAM_user_idx = UAM.indices \n",
    "    UAM_artist_idx = UAM.indptr \n",
    "    return UAM, UAM_user_idx, UAM_artist_idx, user_ids, artist_ids\n",
    "\n",
    "def entropy_label_distribution(labels):\n",
    "    n_labels = len(labels)\n",
    "    if n_labels <= 1:\n",
    "        return 0\n",
    "    value, counts = np.unique(labels, return_counts=True)\n",
    "    probs = counts / np.float32(n_labels)\n",
    "    n_classes = np.count_nonzero(probs)\n",
    "    if n_classes <= 1:\n",
    "        return 0.0\n",
    "    ent = 0.0\n",
    "    for p in probs:\n",
    "        ent -= p * np.log(p)\n",
    "    return ent\n",
    "\n",
    "def divide_column_into_user_groups_and_merge(df, column_name):\n",
    "    sorted_values = np.sort(df[column_name].values)\n",
    "    third_of_a_sum = sum(sorted_values[~np.isnan(sorted_values)])/3\n",
    "    is_first_third_threshold_set = False\n",
    "    is_second_third_threshold_set = False\n",
    "    cummulative_sum = 0\n",
    "    for value in sorted_values:\n",
    "        if np.isnan(value):\n",
    "            continue\n",
    "        if cummulative_sum > third_of_a_sum and is_first_third_threshold_set==False:\n",
    "            first_third_threshold = value\n",
    "            is_first_third_threshold_set = True\n",
    "        if cummulative_sum > 2*third_of_a_sum and is_second_third_threshold_set==False:\n",
    "            second_third_threshold = value\n",
    "            is_second_third_threshold_set = True\n",
    "        cummulative_sum += value\n",
    "    user_groups = []\n",
    "    for value in df[column_name].values:\n",
    "        if np.isnan(value):\n",
    "            user_groups.append(None)\n",
    "        elif value <= first_third_threshold:\n",
    "            user_groups.append('low')\n",
    "        elif first_third_threshold < value <= second_third_threshold:\n",
    "            user_groups.append('medium')\n",
    "        elif value > second_third_threshold:\n",
    "            user_groups.append('high')\n",
    "    user_groups_df = pd.DataFrame(user_groups).rename(columns={0:'user_groups_'+column_name})\n",
    "    df = pd.concat([df, user_groups_df], axis=1)\n",
    "    return df\n",
    "\n",
    "def fill_missing_values(users_df):\n",
    "    columns_to_be_replaced_with_mean = []\n",
    "    for column in users_df.columns:\n",
    "        if users_df[column].isna().any():\n",
    "            users_df[column] = users_df[column].fillna(users_df[column].mean())\n",
    "    return users_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfm1b_user_info_filepath = '../data/raw/LFM-1b_social/LFM-1b_users.txt'\n",
    "lfm1b_user_additional_info_filepath = '../data/raw/LFM-1b/LFM-1b_users_additional.txt'\n",
    "lfm1b_user_genres_allmusic_filepath = '../data/raw/LFM-1b_UGP/LFM-1b_UGP_weightedPC_allmusic.txt'\n",
    "lfm1b_user_genres_freebase_filepath = '../data/raw/LFM-1b_UGP/LFM-1b_UGP_weightedPC_freebase.txt'\n",
    "input_edgelist_csv_filepath = '../data/raw/LFM-1b_social/LFM-1b_social_ties.txt'\n",
    "lfm1b_user_artists_LEs_filepath = '../data/raw/LFM-1b/LFM-1b_LEs.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and unpacking LFM-1b dataset (~8GB)...\n",
      "100% [....................................................................] 8672428478 / 8672428478Finished.\n",
      "Downloading and unpacking LFM-1b_UGP dataset (~166MB)...\n",
      "100% [......................................................................] 170935289 / 170935289Finished.\n",
      "Downloading and unpacking LFM-1b_social dataset (~2MB)...\n",
      "100% [..........................................................................] 1786815 / 1786815Finished.\n"
     ]
    }
   ],
   "source": [
    "prepare_directories()\n",
    "download_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NMF] Running: \n",
      "{\n",
      "    \"A_dim_1\": 120175,\n",
      "    \"A_dim_2\": 1999,\n",
      "    \"A_type\": \"<class 'scipy.sparse.csr.csr_matrix'>\",\n",
      "    \"alg\": \"<class 'nonnegfac.nmf.NMF_ANLS_BLOCKPIVOT'>\",\n",
      "    \"init\": \"uniform_random\",\n",
      "    \"k\": 20,\n",
      "    \"max_iter\": 100,\n",
      "    \"max_time\": Infinity,\n",
      "    \"verbose\": 0\n",
      "}\n",
      "[NMF] Completed: \n",
      "{\n",
      "    \"elapsed\": 334.2760293483734,\n",
      "    \"iterations\": 100,\n",
      "    \"norm_A\": 346.868402972796,\n",
      "    \"rel_error\": 0.0031916893330365516\n",
      "}\n",
      "[NMF] Running: \n",
      "{\n",
      "    \"A_dim_1\": 120175,\n",
      "    \"A_dim_2\": 585095,\n",
      "    \"A_type\": \"<class 'scipy.sparse.csr.csr_matrix'>\",\n",
      "    \"alg\": \"<class 'nonnegfac.nmf.NMF_ANLS_BLOCKPIVOT'>\",\n",
      "    \"init\": \"uniform_random\",\n",
      "    \"k\": 20,\n",
      "    \"max_iter\": 100,\n",
      "    \"max_time\": Infinity,\n",
      "    \"verbose\": 0\n",
      "}\n",
      "[NMF] Completed: \n",
      "{\n",
      "    \"elapsed\": 1317.9680819511414,\n",
      "    \"iterations\": 100,\n",
      "    \"norm_A\": 833.7120940073838,\n",
      "    \"rel_error\": 0.9458134092364059\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "edgelist_df = pd.read_csv('../data/raw/LFM-1b_social/LFM-1b_social_ties.txt', sep='\\t')\n",
    "users_df_without_missing_values = create_users_df(edgelist_df,\n",
    "                                                  lfm1b_user_info_filepath,\n",
    "                                                  lfm1b_user_additional_info_filepath,\n",
    "                                                  lfm1b_user_genres_allmusic_filepath,\n",
    "                                                  True)\n",
    "\n",
    "users_df_without_missing_values = create_users_df(edgelist_df,\n",
    "                                                  lfm1b_user_info_filepath,\n",
    "                                                  lfm1b_user_additional_info_filepath,\n",
    "                                                  lfm1b_user_genres_allmusic_filepath,\n",
    "                                                  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "users_df = pd.read_csv('../data/dataframes/users_dfs/users_df.csv')\n",
    "users_df_without_missing_values = pd.read_csv('../data/dataframes/users_dfs/users_df_no_missing_values.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
